{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010f9f7d",
   "metadata": {},
   "source": [
    "### About this script\n",
    "This script is used as a 'running experiment' to scrape papers data from various sources such as Google Scholar, SemanticScholar, Crossref and ResearchGate: therefore, expect some spaghetti. The scraping pipeline is made out of several (some technically optional) stages:\n",
    "\n",
    "1. Define the root paper (in this case, it will be X. Leroy's Coinductive big-step operational semantics).\n",
    "2. [optional] Scrape *Google Scholar* and/or *Semantic Scholar* to get papers that cite the root paper.\n",
    "3. [optional] Fill part of the missing details querying *Crossref*'s REST API, *ResearchGate* and/or *ACM*.\n",
    "5. Produce a set of complete papers.\n",
    "    \n",
    "Once these steps are completed, we should have a mostly complete set of papers: of course, we aren't guaranteed that every papers has a valid DOI, abstract and so on, so manual adjusting might still be necessary. Once we have a complete dataset, we proceed to compute some statistics about the papers, i.e. *tf-ifd* on the documents' abstract; then, we have a list of relevant terms which we can order and show in relation to the papers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0e41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Article:\n",
    "    author: str\n",
    "    title: str\n",
    "    doi: Optional[str]\n",
    "    abstract: Optional[str]\n",
    "    url: Optional[str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        author: str,\n",
    "        title: str,\n",
    "        doi: Optional[str] = None,\n",
    "        abstract: Optional[str] = None,\n",
    "        url: Optional[str] = None,\n",
    "    ):\n",
    "        self.author = author\n",
    "        self.title = title\n",
    "        self.doi = doi\n",
    "        self.abstract = abstract\n",
    "        self.url = url\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "\n",
    "# Uncomment to clutter your screen\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "base = Article(\n",
    "    author=\"Xavier Leroy\",\n",
    "    title=\"Coinductive big-step operational semantics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1cae0",
   "metadata": {},
   "source": [
    "The following block controls the program: scrape or not, check Crossref or not, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d74bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cits_path = \"./cits.json\"\n",
    "\n",
    "# (Re)do scraping or not. \n",
    "scrape = True\n",
    "\n",
    "# Scrape Google Scholar\n",
    "do_gs = scrape and True\n",
    "# Scrape SemanticScholar\n",
    "do_s2 = scrape and True\n",
    "\n",
    "# (Re)do filling or not. \n",
    "fill = False\n",
    "\n",
    "# Scrape Crossref\n",
    "do_cf = fill and False\n",
    "# Scrape ResearchGate\n",
    "do_rg = fill and False\n",
    "# Scrape ACM\n",
    "do_acm = fill and False\n",
    "\n",
    "# Do statistical analysis. \n",
    "stats = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a47821",
   "metadata": {},
   "source": [
    "### Scrape Google Scholar and/or Semantic Scholar to get papers that cite the root paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a17d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "cits = {(base.title, base)}\n",
    "\n",
    "if not scrape: \n",
    "    # Assume we already have a file to read citations from. \n",
    "    try:\n",
    "        with open(cits_path) as f:\n",
    "            articles = json.load(f)\n",
    "            for article in articles:\n",
    "                cits[article[\"title\"]] = Article(\n",
    "                    author=article[\"author\"],\n",
    "                    title=article[\"title\"],\n",
    "                    doi=article[\"doi\"],\n",
    "                    abstract=article[\"abstract\"],\n",
    "                    url = article[\"url\"]\n",
    "                )\n",
    "    except BaseException:\n",
    "        raise\n",
    "else:\n",
    "    # This will take a lot of time.\n",
    "    if do_gs:\n",
    "        # google scholar scraper\n",
    "        from scholarly import scholarly\n",
    "\n",
    "        # Avoid Google CAPTCHAs...\n",
    "        from scholarly import ProxyGenerator\n",
    "\n",
    "        pg = ProxyGenerator()\n",
    "        \n",
    "        # Change here if you don't want to use Tor.\n",
    "        use_tor = True\n",
    "        \n",
    "        if use_tor:\n",
    "            success = pg.Tor_Internal(tor_cmd=\"tor\")\n",
    "            if not success:\n",
    "                print(\"Tor not working...\")\n",
    "                raise\n",
    "        else:\n",
    "            pg.FreeProxies()\n",
    "            import ssl\n",
    "\n",
    "            ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "        scholarly.use_proxy(pg)\n",
    "\n",
    "        gs_base = scholarly.search_single_pub(base.title)\n",
    "        logging.info(\n",
    "            \"fetched gscholar pub for base article \" +\n",
    "            str(base) +\n",
    "            \" : \" +\n",
    "            str(gs_base))\n",
    "        assert gs_base[\"bib\"][\"title\"] == base.title\n",
    "\n",
    "        p\n",
    "        for cit in scholarly.citedby(gs_base):\n",
    "            logging.info(\n",
    "                \"citation \"\n",
    "                + str(cit[\"bib\"][\"author\"][0])\n",
    "                + \" \"\n",
    "                + cit[\"bib\"][\"title\"]\n",
    "                + \" cited base\"\n",
    "            )\n",
    "            gs_cits.append(cit)\n",
    "\n",
    "        for cit in gs_cits:\n",
    "            author, title = cit[\"bib\"][\"author\"][0], cit[\"bib\"][\"title\"]\n",
    "            cits[title] = Article(author=author, title=title)\n",
    "\n",
    "    if do_s2:\n",
    "        import s2\n",
    "        import requests\n",
    "\n",
    "        res = requests.get(\n",
    "            \"https://api.semanticscholar.org/graph/v1/paper/search?query=\"\n",
    "            + base.title.replace(\" \", \"+\").replace(\"-\", \"+\")\n",
    "            + \"&limit=1\"\n",
    "        )\n",
    "        s2_base = json.loads(res.text)\n",
    "        logging.info(\n",
    "            \"fetched s2 pub for base article \" +\n",
    "            str(base) +\n",
    "            \" : \" +\n",
    "            str(s2_base))\n",
    "        s2_base = s2_base[\"data\"][0]\n",
    "        assert s2_base[\"title\"] == base.title\n",
    "        s2_base = s2.api.get_paper(paperId=s2_base[\"paperId\"])\n",
    "        base.doi = s2_base.doi\n",
    "        for cit in s2_base.citations:\n",
    "            author, doi, title = (\n",
    "                cit.authors[0].name,\n",
    "                cit.doi,\n",
    "                cit.title,\n",
    "            )\n",
    "            logging.info(\n",
    "                \"cit \" +\n",
    "                str(author) +\n",
    "                \" \" +\n",
    "                str(title) +\n",
    "                \" cited base\")\n",
    "            cits[title] = Article(author=author, doi=doi, title=title)\n",
    "\n",
    "    with open(cits_path, \"w+\") as f:\n",
    "        json.dump([cit.__dict__ for cit in cits.values()], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ab79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill and do_cf:\n",
    "    # crossref.org client\n",
    "    import habanero\n",
    "\n",
    "    cr = habanero.Crossref()\n",
    "\n",
    "    # Specify email to get into the 'polite' queue\n",
    "    user_mail = \"ecmm@anche.no\"\n",
    "    habanero.Crossref(mailto=user_mail)\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "\n",
    "    lastit = 1\n",
    "\n",
    "    with logging_redirect_tqdm():\n",
    "        while lastit < len(cits.values()):\n",
    "            i = 0\n",
    "            try:\n",
    "                for cit in tqdm(cits.values()):\n",
    "                    i = i + 1\n",
    "                    if i <= lastit:\n",
    "                        continue\n",
    "                    if cit.doi is None or cit.abstract is None:\n",
    "                        do_extsearch = True\n",
    "                        if cit.doi is not None:\n",
    "                            try:\n",
    "                                res = cr.works(ids=cit.doi)\n",
    "                                if not res[\"status\"] == \"ok\":\n",
    "                                    logging.error(\n",
    "                                        \"Error fetching crossref work for citation \"\n",
    "                                        + str(cit)\n",
    "                                    )\n",
    "                                    continue\n",
    "                                item = res[\"message\"]\n",
    "                                author = item[\"author\"][0]\n",
    "                                author = author[\"given\"] + \" \" + author[\"family\"]\n",
    "                                if len(author) > len(cit.author):\n",
    "                                    cit.author = author\n",
    "                                abstract = None\n",
    "                                if \"abstract\" in item:\n",
    "                                    abstract = item[\"abstract\"]\n",
    "                                if cit.abstract is None:\n",
    "                                    cit.abstract = abstract\n",
    "                                do_extsearch = False\n",
    "                            except BaseException as e:\n",
    "                                logging.error(str(e))\n",
    "                                cit.doi = None\n",
    "                                pass\n",
    "\n",
    "                        if do_extsearch:\n",
    "                            res = cr.works(\n",
    "                                query=cit.title,\n",
    "                            )\n",
    "\n",
    "                            res1 = cr.works(\n",
    "                                query=cit.title + \" \" + cit.author,\n",
    "                                query_author=cit.author,\n",
    "                            )\n",
    "\n",
    "                            res2 = cr.works(\n",
    "                                query_title=cit.title,\n",
    "                            )\n",
    "\n",
    "                            res3 = cr.works(\n",
    "                                query_container_title=cit.title,\n",
    "                            )\n",
    "\n",
    "                            items = (\n",
    "                                res[\"message\"][\"items\"]\n",
    "                                + res1[\"message\"][\"items\"]\n",
    "                                + res2[\"message\"][\"items\"]\n",
    "                                + res3[\"message\"][\"items\"]\n",
    "                            )\n",
    "\n",
    "                            for item in items:\n",
    "                                try:\n",
    "                                    title = item[\"title\"][0]\n",
    "                                    if not title.lower() == cit.title.lower():\n",
    "                                        continue\n",
    "                                    if \"DOI\" not in item:\n",
    "                                        continue\n",
    "                                    doi = item[\"DOI\"]\n",
    "                                    abstract = None\n",
    "                                    author = item[\"author\"][0]\n",
    "                                    author = author[\"given\"] + \" \" + author[\"family\"]\n",
    "                                    if len(author) > len(cit.author):\n",
    "                                        cit.author = author\n",
    "                                    if \"abstract\" in item:\n",
    "                                        abstract = item[\"abstract\"]\n",
    "                                    if cit.doi is None:\n",
    "                                        cit.doi = doi\n",
    "                                    if cit.abstract is None:\n",
    "                                        cit.abstract = abstract\n",
    "                                except BaseException:\n",
    "                                    pass\n",
    "                            lastit = i\n",
    "            except:\n",
    "                lastit = i\n",
    "                continue\n",
    "\n",
    "    with open(cits_path, \"w+\") as f:\n",
    "        to_write = [cit.__dict__ for cit in cits.values()]\n",
    "        json.dump(to_write, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db59524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill and do_rg:\n",
    "    from bs4 import BeautifulSoup\n",
    "    import selenium\n",
    "    import time\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "\n",
    "    browser = webdriver.Firefox()\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "    import re\n",
    "\n",
    "    session = browser\n",
    "\n",
    "    use_tor = False\n",
    "    if use_tor:\n",
    "        from stem import Signal\n",
    "        from stem.control import Controller\n",
    "\n",
    "        # signal TOR for a new connection\n",
    "        def renew_connection():\n",
    "            with Controller.from_port(port=9051) as controller:\n",
    "                # controller.authenticate(password=\"password\")\n",
    "                controller.signal(Signal.NEWNYM)\n",
    "\n",
    "        def get_tor_session():\n",
    "            session = requests.session()\n",
    "            # Tor uses the 9050 port as the default socks port\n",
    "            session.proxies = {\n",
    "                \"http\": \"socks5://127.0.0.1:9050\",\n",
    "                \"https\": \"socks5://127.0.0.1:9050\",\n",
    "            }\n",
    "            return session\n",
    "\n",
    "        session = get_tor_session()\n",
    "\n",
    "    lastit = 1\n",
    "    with logging_redirect_tqdm():\n",
    "        while lastit < len(cits.values()):\n",
    "            logging.info(\"lastit: \" + str(lastit))\n",
    "            i = 0\n",
    "\n",
    "            try:\n",
    "                for cit in tqdm(cits.values()):\n",
    "                    \n",
    "                    i = i + 1\n",
    "                    if i <= lastit:\n",
    "                        continue\n",
    "                    time.sleep(1)\n",
    "                    url = f\"https://www.researchgate.net/search/publication?q={cit.title}+{cit.author}\".replace(\n",
    "                        \" \", \"+\"\n",
    "                    )\n",
    "                    session.get(url)\n",
    "                    res = browser.page_source\n",
    "                    soup = BeautifulSoup(res, \"html.parser\")\n",
    "                    for article in soup.find_all(\n",
    "                        \"div\", {\"class\": \"nova-legacy-v-publication-item__body\"}\n",
    "                    ):\n",
    "                        title_tag = article.find_all(\n",
    "                            \"a\", {\"class\": \"nova-legacy-e-link--color-inherit\"}\n",
    "                        )\n",
    "                        title = None\n",
    "                        rg_url = None\n",
    "                        doi = None\n",
    "                        if len(title_tag) > 0:\n",
    "                            title_tag = title_tag[0]\n",
    "                            title = title_tag.text\n",
    "                            rg_url = \"https://www.researchgate.net/search/../\" + title_tag[\"href\"]\n",
    "                        for bar in article.find_all(\n",
    "                            \"ul\", {\"class\": \"nova-legacy-e-list--type-inline\"}\n",
    "                        ):\n",
    "                            for item in bar.find_all(\"li\"):\n",
    "                                if \"DOI\" in item.text:\n",
    "                                    doi = item.text.strip(\"DOI: \")\n",
    "                                    \n",
    "                        if title.lower() == cit.title.lower():\n",
    "                            session.get(rg_url)\n",
    "                            res = browser.page_source\n",
    "                            soup = BeautifulSoup(res, \"html.parser\")\n",
    "                            abstract = soup.find(\"div\", {\"class\": \"research-detail-middle-section__abstract\"}).text\n",
    "                            if cit.doi is None: \n",
    "                                cit.doi = doi\n",
    "                            if cit.abstract is None: \n",
    "                                cit.abstract = abstract\n",
    "                        lastit = i\n",
    "            except BaseException as e:\n",
    "                print(e)\n",
    "                lastit = i\n",
    "                continue\n",
    "    with open(cits_path, \"w+\") as f:\n",
    "        to_write = [cit.__dict__ for cit in cits.values()]\n",
    "        json.dump(to_write, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee65eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill and do_acm: \n",
    "    from bs4 import BeautifulSoup\n",
    "    import selenium\n",
    "    import time\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "\n",
    "    browser = webdriver.Firefox()\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "    import re\n",
    "\n",
    "    session = browser\n",
    "\n",
    "    use_tor = False\n",
    "    if use_tor:\n",
    "        from stem import Signal\n",
    "        from stem.control import Controller\n",
    "\n",
    "        # signal TOR for a new connection\n",
    "        def renew_connection():\n",
    "            with Controller.from_port(port=9051) as controller:\n",
    "                # controller.authenticate(password=\"password\")\n",
    "                controller.signal(Signal.NEWNYM)\n",
    "\n",
    "        def get_tor_session():\n",
    "            session = requests.session()\n",
    "            # Tor uses the 9050 port as the default socks port\n",
    "            session.proxies = {\n",
    "                \"http\": \"socks5://127.0.0.1:9050\",\n",
    "                \"https\": \"socks5://127.0.0.1:9050\",\n",
    "            }\n",
    "            return session\n",
    "\n",
    "        session = get_tor_session()\n",
    "\n",
    "    lastit = 1\n",
    "    with logging_redirect_tqdm():\n",
    "        while lastit < len(cits.values()):\n",
    "            logging.info(\"lastit: \" + str(lastit))\n",
    "            i = 0\n",
    "\n",
    "            try:\n",
    "                for cit in tqdm(cits.values()):\n",
    "                    \n",
    "                    i = i + 1\n",
    "                    if i <= lastit:\n",
    "                        continue\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    url = f\"https://dl.acm.org/action/doSearch?AllField={cit.title}+{cit.author}&startPage=0&pageSize=50\".replace(\n",
    "                        \" \", \"+\"\n",
    "                    )\n",
    "                    session.get(url)\n",
    "                    res = browser.page_source\n",
    "                    soup = BeautifulSoup(res, \"html.parser\")\n",
    "                    for article in soup.find_all(\n",
    "                        \"li\", {\"class\": \"search__item issue-item-container\"}\n",
    "                    ):\n",
    "                        title_tag = article.find_all(\n",
    "                            \"span\", {\"class\": \"hlFld-Title\"}\n",
    "                        )\n",
    "                        title = None\n",
    "                        rg_url = None\n",
    "                        doi = None\n",
    "                        if len(title_tag) > 0:\n",
    "                            title_tag = title_tag[0]\n",
    "                            title = title_tag.text\n",
    "                            rg_url = \"https://dl.acm.org\" + title_tag.find(\"a\", href=True)[\"href\"]\n",
    "                        doi_tag = article.find(\"a\", {\"class\":\"issue-item__doi\"})\n",
    "                        if doi_tag is not None: \n",
    "                            doi = doi_tag[\"href\"]\n",
    "                        if title is None: \n",
    "                            continue\n",
    "                        if title.lower() == cit.title.lower():\n",
    "                            session.get(rg_url)\n",
    "                            res = browser.page_source\n",
    "                            soup = BeautifulSoup(res, \"html.parser\")\n",
    "                            abstract = soup.find(\"div\", {\"class\": \"abstractSection\"}).text\n",
    "                            if cit.doi is None: \n",
    "                                cit.doi = doi\n",
    "                            if cit.abstract is None: \n",
    "                                cit.abstract = abstract\n",
    "                        lastit = i\n",
    "            except BaseException as e:\n",
    "                print(e)\n",
    "                lastit = i\n",
    "                continue\n",
    "    with open(cits_path, \"w+\") as f:\n",
    "        to_write = [cit.__dict__ for cit in cits.values()]\n",
    "        json.dump(to_write, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01edba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without DOI: 155\n",
      "without abstract: 161\n"
     ]
    }
   ],
   "source": [
    "print(\"without DOI:\", len(list(filter(lambda x: x.doi is None, cits.values()))))\n",
    "print(\"without abstract:\", len(list(filter(lambda x: x.abstract is None, cits.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "112b3747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 144 documents w/abstract in the dataset\n",
      "Flexible coinduction in agda 0.16996154584003076\n",
      "Flexible coinduction in Agda 0.16996154584003076\n",
      "Flexible Coinduction in Agda 0.16996154584003076\n",
      "Operational semantics using the partiality monad 0.11423780231969859\n",
      "Beating the Productivity Checker Using Embedded Languages 0.07988117770413543\n",
      "Beating the productivity checker using embedded languages 0.07934553190445046\n",
      "Introduction to bisimulation and coinduction 0.0\n",
      "CakeML: a verified implementation of ML 0.0\n",
      "A verified compiler for an impure functional language 0.0\n",
      "Biorthogonality, step-indexing and compiler correctness 0.0\n",
      "Probabilistic operational semantics for the lambda calculus 0.0\n",
      "Functional big-step semantics 0.0\n",
      "One-path reachability logic 0.0\n",
      "Dynamic determinacy analysis 0.0\n",
      "Pretty-big-step semantics 0.0\n",
      "Type soundness proofs with definitional interpreters 0.0\n",
      "Interaction trees: representing recursive and impure programs in Coq 0.0\n",
      "Denotational cost semantics for functional languages with inductive types 0.0\n",
      "Programming with binders and indexed data-types 0.0\n",
      "Trace-based coinductive operational semantics for while 0.0\n",
      "Resumptions, weak bisimilarity and big-step semantics for while with interactive I/O: An exercise in mixed induction-coinduction 0.0\n",
      "A certified multi-prover verification condition generator 0.0\n",
      "Deepfuzz: Automatic generation of syntax valid c programs for fuzz testing 0.0\n",
      "A static cost analysis for a higher-order language 0.0\n",
      "Towards a unified theory of operational and axiomatic semantics 0.0\n",
      "Compiler verification meets cross-language linking via data abstraction 0.0\n",
      "A Hoare logic for the coinductive trace-based big-step semantics of While 0.0\n",
      "Proof-producing synthesis of ML from higher-order logic 0.0\n",
      "Regular corecursion in Prolog 0.0\n",
      "Generalizing inference systems by coaxioms 0.0\n",
      "Idealized coinductive type systems for imperative object-oriented programs 0.0\n",
      "Producing certified functional code from inductive specifications 0.0\n",
      "Corecursive featherweight java 0.0\n",
      "Sound and complete subtyping between coinductive types for object-oriented languages 0.0\n",
      "A compositional semantics for verified separate compilation and linking 0.0\n",
      "A coinductive confluence proof for infinitary lambda-calculus 0.0\n",
      "Modular relaxed dependencies in weak memory concurrency 0.0\n",
      "Certified abstract interpretation with pretty-big-step semantics 0.0\n",
      "A decision procedure for univariate real polynomials in Isabelle/HOL 0.0\n",
      "Deriving pretty-big-step semantics from small-step semantics 0.0\n",
      "Soundness of object-oriented languages with coinductive big-step semantics 0.0\n",
      "Program sketching with live bidirectional evaluation 0.0\n",
      "Multi-ML: programming multi-BSP algorithms in ML 0.0\n",
      "Mechanized semantics and verified compilation for a dataflow synchronous language with reset 0.0\n",
      "A supposedly fun thing I may have to do again: A HOAS encoding of Howe's method 0.0\n",
      "Reasoning on divergent computations with coaxioms 0.0\n",
      "Tools and techniques for the verification of modular stateful code 0.0\n",
      "Local temporal reasoning 0.0\n",
      "Certification of a tool chain for deductive program verification 0.0\n",
      "From small-step semantics to big-step semantics, automatically 0.0\n",
      "Semantic subtyping for imperative object-oriented languages 0.0\n",
      "How to prove type soundness of Java-like languages without forgoing big-step semantics 0.0\n",
      "Flag-based big-step semantics 0.0\n",
      "Structural operational semantics through context-dependent behaviour 0.0\n",
      "Formal small-step verification of a call-by-value lambda calculus machine 0.0\n",
      "Transfinite semantics in the form of greatest fixpoint 0.0\n",
      "Automated amortised analysis 0.0\n",
      "Certification of an instruction set simulator 0.0\n",
      "A coinductive semantics of the unlimited register machine 0.0\n",
      "Foundations of regular coinduction 0.0\n",
      "Semantics of an intermediate language for program transformation 0.0\n",
      "Flexible coinductive logic programming 0.0\n",
      "Compositional optimizations for certicoq 0.0\n",
      "Two formal semantics of a subset of the paderborn university bsplib 0.0\n",
      "Coinductive big-step operational semantics for type soundness of Java-like languages 0.0\n",
      "Modular, compositional, and executable formal semantics for LLVM IR 0.0\n",
      "Integrating induction and coinduction via closure operators and proof cycles 0.0\n",
      "Proving fixed points 0.0\n",
      "A big step from finite to infinite computations 0.0\n",
      "Coinductive big-step semantics for concurrency 0.0\n",
      "Verification of Programs with Pointers in SPARK 0.0\n",
      "A theory of agreements and protection 0.0\n",
      "Squeezing Streams and Composition of Self-stabilizing Algorithms 0.0\n",
      "RML: Theory and practice of a domain specific language for runtime verification 0.0\n",
      "Infinitary lambda calculi from a linear perspective 0.0\n",
      "A Certified Extension of the Krivine Machine for a Call-by-Name Higher-Order Imperative Language 0.0\n",
      "Formal verifications of call-by-need and call-by-name evaluations with mutual recursion 0.0\n",
      "A Comparison of Big-step Semantics Definition Styles 0.0\n",
      "Imperative polymorphism by store-based types as abstract interpretations 0.0\n",
      "A logical correspondence between natural semantics and abstract machines 0.0\n",
      "Coinductive Natural Semantics for Compiler Verification in Coq 0.0\n",
      "Position paper: Towards transparent machine learning 0.0\n",
      "Order Theory for Big-Step Semantics 0.0\n",
      "A Correct Compiler from Mini-ML to a Big-Step Machine Verified Using Natural Semantics in Coq 0.0\n",
      "One Step at a Time 0.0\n",
      "A multimedia information server with mixed workload scheduling 0.0\n",
      "A Meta-theory for Big-step Semantics 0.0\n",
      "Partiality, Revisited 0.0\n",
      "Habilitation thesis 0.0\n",
      "Great-Step Semantics 0.0\n",
      "Improved Test Solutions for COTS-Based Systems in Space Applications 0.0\n",
      "Shall we juggle, coinductively? 0.0\n",
      "A framework for big-step semantics 0.0\n",
      "Modular, Compositional, and Executable Formal Semantics for LLVM IR 0.0\n",
      "Syntactic Proofs of Compositional Compiler Correctness 0.0\n",
      "Fixed-Point vs Proof Theoretic Approach to Inference Systems 0.0\n",
      "Coinductive Subtyping for Recursive and Union Types 0.0\n",
      "A Provably Correct Compilation of Functional Languages into Scripting Languages 0.0\n",
      "Breaking boundaries between programming languages and databases 0.0\n",
      "Sémantiques formelles 0.0\n",
      "Compositional optimizations for CertiCoq 0.0\n",
      "Non-well-founded Deduction for Induction and Coinduction 0.0\n",
      "Modular Relaxed Dependencies in Weak Memory Concurrency 0.0\n",
      "Soundness Conditions for Big-Step Semantics 0.0\n",
      "Position Paper: Towards Transparent Machine Learning 0.0\n",
      "Program Synthesis with Live Bidirectional Evaluation 0.0\n",
      "DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing 0.0\n",
      "Formal Small-step Verification of a Call-by-value Lambda Calculus Machine 0.0\n",
      "Generalizing Inference Systems by Coaxioms 0.0\n",
      "Partiality, Revisited: The Partiality Monad as a Quotient Inductive-Inductive Type 0.0\n",
      "Finding compiler bugs via live code mutation 0.0\n",
      "Towards Turing computability via coinduction 0.0\n",
      "Scopes Describe Frames: A Uniform Model for Memory Layout in Dynamic Semantics (Artifact) 0.0\n",
      "Infinitary $\\lambda$-Calculi from a Linear Perspective (Long Version) 0.0\n",
      "Well-founded Functions and Extreme Predicates in Dafny: A Tutorial 0.0\n",
      "Type-based amortized resource analysis with integers and arrays* 0.0\n",
      "Certified Abstract Interpretation with Pretty-Big-Step Semantics 0.0\n",
      "Imperative Polymorphism by Store-Based Types as Abstract Interpretations 0.0\n",
      "Sound and Complete Subtyping between Coinductive Types for Object-Oriented Languages 0.0\n",
      "Type-Based Amortized Resource Analysis with Integers and Arrays 0.0\n",
      "Deriving Pretty-Big-Step Semantics from Small-Step Semantics 0.0\n",
      "Coinductive Big-Step Semantics for Concurrency 0.0\n",
      "Certification of an Instruction Set Simulator 0.0\n",
      "One-Path Reachability Logic 0.0\n",
      "From Small-Step Semantics to Big-Step Semantics, Automatically 0.0\n",
      "Pretty-Big-Step Semantics 0.0\n",
      "A Theory of Agreements and Protection 0.0\n",
      "Producing Certified Functional Code from Inductive Specifications 0.0\n",
      "Multivariate amortized resource analysis 0.0\n",
      "A coinductive semantics of the Unlimited Register Machine 0.0\n",
      "Grammar semantics, analysis and parsing by abstract interpretation 0.0\n",
      "On the reaction time of some synchronous systems 0.0\n",
      "Resumptions, Weak Bisimilarity and Big-Step Semantics for While with Interactive I/O: An Exercise in Mixed Induction-Coinduction 0.0\n",
      "Transfinite Semantics in the Form of Greatest Fixpoint 0.0\n",
      "A Formally Verified Compiler Back-end 0.0\n",
      "Bi-inductive structural semantics 0.0\n",
      "Certificate translation for optimizing compilers 0.0\n",
      "Préservation des preuves et transformation de programmes 0.0\n",
      "Translation Correctness for First-Order Object-Oriented Pattern Matching 0.0\n",
      "Intuitionistic Refinement Calculus 0.0\n",
      "Varieties of Static Analyzers: A Comparison with ASTREE 0.0\n",
      "Structural Operational Semantics 0.0\n",
      "Separation Logic for Small-step Cminor (extended version) 0.0\n",
      "Object-oriented pattern matching 0.0\n"
     ]
    }
   ],
   "source": [
    "if stats:\n",
    "    w_abstract = [cit for cit in cits.values() if cit.abstract is not None]\n",
    "    print(\"there are\" , len(w_abstract), \"documents w/abstract in the dataset\")\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity \n",
    "    import nltk \n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    import string \n",
    "    \n",
    "    stop  = set(stopwords.words('english') + list(string.punctuation))\n",
    "    \n",
    "    tokenizer = lambda x: [i for i in nltk.word_tokenize(x.lower()) if i not in stop]\n",
    "    vectorizer = TfidfVectorizer(tokenizer = tokenizer)\n",
    "    documents = vectorizer.fit_transform([c.abstract for c in w_abstract])\n",
    "    \n",
    "    def search(query): \n",
    "        q = vectorizer.transform([query])\n",
    "        match = cosine_similarity(q, documents)\n",
    "        answers, scores = [], []\n",
    "        for i, s in sorted(enumerate(match[0]), key=lambda x: -x[1]):\n",
    "            answers.append(i)\n",
    "            scores.append(s)\n",
    "        return answers, scores\n",
    "    \n",
    "    anss, scores = search(\"agda\")\n",
    "    for i in range(len(anss)): \n",
    "        print(w_abstract[ans[i]].title, scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13849cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Big Step from Finite to Infinite Computations (SCICO Journal-first)\n",
      "A Certified Extension of the Krivine Machine for a Call-by-Name Higher-Order Imperative Language\n",
      "A Comparison of Big-step Semantics Definition Styles\n",
      "A Compositional Framework for Certified Separate Compilation and Modular Program Verification\n",
      "A Correct Compiler from Mini-ML to a Big-Step Machine Verified Using Natural Semantics in Coq\n",
      "A Formally Verified Compiler Back-end\n",
      "A Hoare logic for the coinductive trace-based big-step semantics of While\n",
      "A Machine-Checked, Type-Safe Model of Java Concurrency\n",
      "A Meta-theory for Big-step Semantics\n",
      "A Mixin Based Object-Oriented Calculus: True Modularity in Object-Oriented Programming\n",
      "A Provably Correct Compilation of Functional Languages into Scripting Languages\n",
      "A Semantic Approach to Machine-Level Software Security\n",
      "A Supposedly Fun Thing I May Have to Do Again\n",
      "A Theory of Agreements and Protection\n",
      "A big step from finite to infinite computations\n",
      "A certified multi-prover verification condition generator\n",
      "A coinductive confluence proof for infinitary lambda-calculus\n",
      "A coinductive effect system for precise global type analysis of object-oriented languages\n",
      "A coinductive semantics of the Unlimited Register Machine\n",
      "A coinductive semantics of the unlimited register machine\n",
      "A compositional semantics for verified separate compilation and linking\n",
      "A decision procedure for univariate real polynomials in Isabelle/HOL\n",
      "A formal semantics of the MULTI-ML language\n",
      "A formally verified compiler back-end\n",
      "A framework for big-step semantics\n",
      "A lambda calculus for transfinite arrays: Unifying arrays and streams\n",
      "A list-machine benchmark for mechanized metatheory\n",
      "A logical correspondence between natural semantics and abstract machines\n",
      "A multimedia information server with mixed workload scheduling\n",
      "A new coinductive confluence proof for infinitary lambda calculus\n",
      "A new coinductive confluence proof for infinitary lambda-calculus\n",
      "A static cost analysis for a higher-order language\n",
      "A supposedly fun thing I may have to do again: A HOAS encoding of Howe's method\n",
      "A theory of agreements and protection\n",
      "A verified compiler for an impure functional language\n",
      "Abstraction fonctionnelle pour la programmation d'architecture multi-niveaux: formalisation et implantation\n",
      "Actes des deuxièmes journées nationales du Groupement de Recherche CNRS du Génie de la Programmation et du Logiciel\n",
      "Advances in Property-Based Testing for $α$Prolog\n",
      "Advances in Property-Based Testing for Prolog\n",
      "Advances in Property-Based Testing for \\alpha Prolog\n",
      "Amortized Resource Analysis with Polymorphic Recursion and Partial Big-Step Operational Semantics\n",
      "Au-delà des frontières entre langages de programmation et bases de données\n",
      "Automated Derivation of Abstract Machines from Reduction Semantics\n",
      "Automated amortised analysis\n",
      "BSP-Why, un outil pour la vérification déductive de programmes BSP: machine-checked semantics and application to distributed state-space algorithms\n",
      "BSP-Why: a Tool for Deductive Verification of BSP Programs\n",
      "Beating the Productivity Checker Using Embedded Languages\n",
      "Beating the productivity checker using embedded languages\n",
      "Bi-inductive Structural Semantics: (Extended Abstract)\n",
      "Bi-inductive structural semantics\n",
      "Big-step Operational Semantics Revisited\n",
      "Big-step operational semantics revisited\n",
      "Biorthogonality, step-indexing and compiler correctness\n",
      "Borrowing Safe Pointers from Rust in SPARK\n",
      "Borrowing safe pointers from rust in spark\n",
      "Breaking boundaries between programming languages and databases\n",
      "CENTRE D'ORSAY\n",
      "CPS Semantics: Smoother Nondeterminism in Operational Semantics\n",
      "CakeML: a verified implementation of ML\n",
      "Certificate translation for optimizing compilers\n",
      "Certification of a Tool Chain for Deductive Program Verification. (Certification d'une chaine de vérification déductive de programmes)\n",
      "Certification of a tool chain for deductive program verification\n",
      "Certification of an Instruction Set Simulator\n",
      "Certification of an instruction set simulator\n",
      "Certified Abstract Interpretation with Pretty-Big-Step Semantics\n",
      "Certified abstract interpretation with pretty-big-step semantics\n",
      "Certifying C program correctness with respect to CompCert with VeriFast\n",
      "Characteristic formulae for liveness properties of non-terminating CakeML programs\n",
      "Circular causality in event structures\n",
      "Co-induction Simply - Automatic Co-inductive Proofs in a Program Verifier\n",
      "Coaxioms: flexible coinductive definitions by inference systems\n",
      "Coinduction: an elementary approach\n",
      "Coinductive Big-Step Semantics for Concurrency\n",
      "Coinductive Definition: A Counterpoint to Inductive Definition\n",
      "Coinductive Natural Semantics for Compiler Verification in Coq\n",
      "Coinductive Subtyping for Recursive and Union Types\n",
      "Coinductive Techniques in Infinitary Lambda-Calculus\n",
      "Coinductive big-step operational semantics for type soundness of Java-like languages\n",
      "Coinductive big-step semantics for concurrency\n",
      "Coinductive program verification\n",
      "Coinductive techniques in infinitary lambda-calculus\n",
      "Combining Analyses for C Program Verification\n",
      "Combining analyses for C program verification\n",
      "Comment gagner confiance en C\n",
      "Compilación Certificada sobre Máquinas Abstractas de evaluación normal\n",
      "Compiler verification meets cross-language linking via data abstraction\n",
      "Completely iterative monads in semantics of coinductive programs\n",
      "Compositional optimizations for CertiCoq\n",
      "Compositional optimizations for certicoq\n",
      "Confluence of nearly orthogonal infinitary term rewriting systems\n",
      "Coq Support in HAHA\n",
      "Corecursion in abstract compilation of object-oriented languages\n",
      "Corecursive featherweight java\n",
      "Cost Analysis of Programs Based on the Refinement of Cost Relations\n",
      "Cost analysis of programs based on the refinement of cost relations\n",
      "Dafny Reference Manual\n",
      "DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing\n",
      "Deepfuzz: Automatic generation of syntax valid c programs for fuzz testing\n",
      "Denotational cost semantics for functional languages with inductive types\n",
      "Dependent Object Types\n",
      "Deriving Practical Implementations of First-Class Functions\n",
      "Deriving Pretty-Big-Step Semantics from Small-Step Semantics\n",
      "Deriving pretty-big-step semantics from small-step semantics\n",
      "Dioïdes et idéaux de polynômes en analyse statique\n",
      "Divergence as state in coinductive big-step semantics\n",
      "Draft Dafny Reference Manual\n",
      "Draft Dafny Reference Manual Manuscript Dafny Reference\n",
      "Draft of PhD thesis: The C standard formalized in Coq\n",
      "Dynamic determinacy analysis\n",
      "Extensible transition system semantics\n",
      "Extraction de code fonctionnel certifié à partir de spécifications inductives\n",
      "Featherweight swift: A core calculus for swift's type system\n",
      "Finding compiler bugs via live code mutation\n",
      "First Year Report\n",
      "Fixed Points and Proof Theory: An Extended Abstract\n",
      "Fixed-Point vs Proof Theoretic Approach to Inference Systems\n",
      "Flag-based big-step semantics\n",
      "Flexible Coinduction for Infinite Behaviour.\n",
      "Flexible Coinduction in Agda\n",
      "Flexible coinduction in Agda\n",
      "Flexible coinduction in agda\n",
      "Flexible coinductive logic programming\n",
      "Flow-Sensitive Types for Whiley\n",
      "Flow-sensitive types for whiley\n",
      "Formal Methods and Software Engineering: 22nd International Conference on Formal Engineering Methods, ICFEM 2020, Singapore, Singapore, March 1–3, 2021, Proceedings\n",
      "Formal Operational Semantics of a Core Imperative BSP language\n",
      "Formal Semantics of a Subset of the Paderborn's BSPlib\n",
      "Formal Small-step Verification of a Call-by-value Lambda Calculus Machine\n",
      "Formal small-step verification of a call-by-value lambda calculus machine\n",
      "Formal verifications of call-by-need and call-by-name evaluations with mutual recursion\n",
      "Formalisation of a frame stack semantics for a Java-like language\n",
      "Formalisme pour la conception haut-niveau et détaillée de systèmes de contrôle-commande critiques\n",
      "Formalisme pour la conception haut-niveau et détaillée de systèmes de contrôle-commande critiques. (Formalism for the high-level design of hard real-time embedded systems)\n",
      "Foundations of regular coinduction\n",
      "From Small-Step Semantics to Big-Step Semantics, Automatically\n",
      "From small-step semantics to big-step semantics, automatically\n",
      "Functional abstraction for programming multi-level architectures : formalisation and implementation. (Abstraction fonctionnelle pour la programmation d'architecture multi-niveaux : formalisation et implantation)\n",
      "Functional abstraction for programming multi-level architectures: formalisation and implementation\n",
      "Functional big-step semantics\n",
      "Functional description extraction of code based on formal semantics of Clight\n",
      "Generalizing Inference Systems by Coaxioms\n",
      "Generalizing inference systems by coaxioms\n",
      "Grammar semantics, analysis and parsing by abstract interpretation\n",
      "Great-Step Semantics\n",
      "Habilitation thesis\n",
      "How to prove type soundness of Java-like languages without forgoing big-step semantics\n",
      "Idealized coinductive type systems for imperative object-oriented programs\n",
      "Imperative Polymorphism by Store-Based Types as Abstract Interpretations\n",
      "Imperative polymorphism by store-based types as abstract interpretations\n",
      "Implementation Strategies for Mutable Value Semantics\n",
      "Implementation Strategies for Mutable Value Semantics.\n",
      "Improved Test Solutions for COTS-Based Systems in Space Applications\n",
      "Inductive and Coinductive Predicate Liftings for Effectful Programs\n",
      "Infinitary $\\lambda$-Calculi from a Linear Perspective (Long Version)\n",
      "Infinitary -Calculi from a Linear Perspective (Long Version)\n",
      "Infinitary lambda calculi from a linear perspective\n",
      "Infinite derivations as failures.\n",
      "Integrating induction and coinduction via closure operators and proof cycles\n",
      "Interaction Trees\n",
      "Interaction trees\n",
      "Interaction trees: representing recursive and impure programs in Coq\n",
      "Interactive Verification of Call-by-Value Functional Programs\n",
      "Introduction to bisimulation and coinduction\n",
      "Intuitionistic Refinement Calculus\n",
      "Java (X)\n",
      "Java (X): A Type-Based Program Analysis Framework\n",
      "Langage de combinateurs pour XML : conception, typage, implantation\n",
      "Langage de combinateurs pour XML Conception Typage Implantation\n",
      "Language Support for Reliable Operating Systems\n",
      "Language support for reliable operating systems\n",
      "Local temporal reasoning\n",
      "Mechanized Semantics for the Clight Subset of the C Language\n",
      "Mechanized Verification of CPS Transformations\n",
      "Mechanized semantics\n",
      "Mechanized semantics - with applications to program proof and compiler verification\n",
      "Mechanized semantics and verified compilation for a dataflow synchronous language with reset\n",
      "Mechanized semantics for compiler verification\n",
      "Mechanized semantics for the Clight subset of the C language\n",
      "Mixing induction and coinduction\n",
      "Modeling infinite behaviour by corules\n",
      "Modular Relaxed Dependencies in Weak Memory Concurrency\n",
      "Modular relaxed dependencies in weak memory concurrency\n",
      "Modular, Compositional, and Executable Formal Semantics for LLVM IR\n",
      "Modular, compositional, and executable formal semantics for LLVM IR\n",
      "Multi-ML: programming multi-BSP algorithms in ML\n",
      "Multivariate amortized resource analysis\n",
      "Neural Program Synthesis for Compiler Fuzzing\n",
      "Non-well-founded Deduction for Induction and Coinduction\n",
      "Non-well-founded Deduction for Induction and Coinduction.\n",
      "Obiektowy j¦ zyk programowania oparty na mixinach Prawdziwa modularno±¢ wj¦ zykach obiektowych\n",
      "Object-oriented pattern matching\n",
      "Omnisemantics: Smoother Handling of Nondeterminism\n",
      "On the Design of Generic Static Analyzers for Modern Imperative Languages\n",
      "On the Proof Theory of Property-Based Testing of Coinductive Specifications, or: PBT to Infinity and beyond\n",
      "On the reaction time of some synchronous systems\n",
      "One Step at a Time\n",
      "One-Path Reachability Logic\n",
      "One-path reachability logic\n",
      "Operational semantics using the partiality monad\n",
      "Order Theory for Big-Step Semantics\n",
      "Partiality, Revisited\n",
      "Partiality, Revisited: The Partiality Monad as a Quotient Inductive-Inductive Type\n",
      "Position Paper: Towards Transparent Machine Learning\n",
      "Position paper: Towards transparent machine learning\n",
      "Pretty-Big-Step Semantics\n",
      "Pretty-big-step semantics\n",
      "Probabilistic operational semantics for the lambda calculus\n",
      "Producing Certified Functional Code from Inductive Specifications\n",
      "Producing certified functional code from inductive specifications\n",
      "Program Synthesis with Live Bidirectional Evaluation\n",
      "Program sketching with live bidirectional evaluation\n",
      "Programmer des applications réparties\n",
      "Programming Language Operational Semantics\n",
      "Programming with binders and indexed data-types\n",
      "Programs and proofs\n",
      "Project-Team ASCOLA ASpect and COmposition LAnguages\n",
      "Project-Team Cristal Type-safe programming , modularity and compilation\n",
      "Project-team Gallium Programming Languages, Types, Compilation and Proofs\n",
      "Proof-producing synthesis of ML from higher-order logic\n",
      "Proving Correctness of a Compiler Using Step-indexed Logical Relations\n",
      "Proving compiler correctness using step-indexed logical relations\n",
      "Proving correctness of a compiler using step-indexed logical relations\n",
      "Proving fixed points\n",
      "Préservation des preuves et transformation de programmes\n",
      "RML: Theory and practice of a domain specific language for runtime verification\n",
      "Reasoning on divergent computations with coaxioms\n",
      "Regular corecursion in Prolog\n",
      "Resumptions, Weak Bisimilarity and Big-Step Semantics for While with Interactive I/O\n",
      "Resumptions, Weak Bisimilarity and Big-Step Semantics for While with Interactive I/O: An Exercise in Mixed Induction-Coinduction\n",
      "Resumptions, weak bisimilarity and big-step semantics for while with interactive I/O: An exercise in mixed induction-coinduction\n",
      "Scopes Describe Frames: A Uniform Model for Memory Layout\n",
      "Scopes Describe Frames: A Uniform Model for Memory Layout in Dynamic Semantics (Artifact)\n",
      "Scopes describe frames: A uniform model for memory layout in dynamic semantics\n",
      "Self-compilation and self-verification\n",
      "Semantic subtyping for imperative object-oriented languages\n",
      "Semantics and logics for signals\n",
      "Semantics of an intermediate language for program transformation\n",
      "Separation Logic for Small-Step cminor\n",
      "Separation Logic for Small-step Cminor (extended version)\n",
      "Shall we juggle, coinductively?\n",
      "Simple formally verified compiler in Lean\n",
      "Sound Regular Corecursion in coFJ\n",
      "Sound and Complete Subtyping between Coinductive Types for Object-Oriented Languages\n",
      "Sound and complete subtyping between coinductive types for object-oriented languages\n",
      "Sound regular corecursion in coFJ\n",
      "Soundness Conditions for Big-Step Semantics\n",
      "Soundness Conditions for Big-Step Semantics.\n",
      "Soundness of object-oriented languages with coinductive big-step semantics\n",
      "Specification of imperative languages using operational semantics in Coq\n",
      "Spécification et vérification de systèmes paramétrés\n",
      "Squeezing Streams and Composition of Self-stabilizing Algorithms\n",
      "Stateful Structural Operational Semantics\n",
      "Structural Operational Semantics\n",
      "Structural and Flow-Sensitive Types for Whiley\n",
      "Structural operational semantics through context-dependent behaviour\n",
      "Substructural Logical Specifications (Third committee draft–missing Theorem 8. 4, Section 10.3)\n",
      "Substructural logical specifications\n",
      "Subtyping, declaratively\n",
      "Subtyping, declaratively: an exercise in mixed induction and coinduction\n",
      "Syntactic Proofs of Compositional Compiler Correctness\n",
      "Synthesis of Verified Architectural Components for Critical Systems Hosted on a Verified Microkernel\n",
      "Synthesis of Verified Architectural Components for Critical Systems Hosted on a Verified Microkernel.\n",
      "Sémantique Mécanisée et Compilation Vérifiée pour un Langage Synchrone à Flots de Données avec Réinitialisation\n",
      "Sémantique mécanisée et compilation vérifiée pour un langage synchrone à flots de données avec réinitialisation\n",
      "Sémantiques formelles\n",
      "Techniques for model construction in separation logic\n",
      "The C standard formalized in Coq\n",
      "The Effect of Transition Granularity in the Model Checking of Reactive Systems\n",
      "The Interpretation and Inter-derivation of Small-step and Big-step Specifications\n",
      "The Magda language: ten years after\n",
      "The logical basis of evaluation order and pattern-matching\n",
      "Theory for Software Verification — Draft\n",
      "Theory for Software Verification—draft January 20, 2009\n",
      "Theory for Software Verification—draft January 25, 2009\n",
      "Tipi globali per reti asincrone: un approccio coinduttivo\n",
      "Tools and techniques for the verification of modular stateful code\n",
      "Total Definitional Interpreters for Looping Programs\n",
      "Total Definitional Interpreters for Time and Space Complexity\n",
      "Toward language-independent program verification\n",
      "Towards Turing computability via coinduction\n",
      "Towards a unified theory of operational and axiomatic semantics\n",
      "Trace-based coinductive operational semantics for while\n",
      "Transfinite Semantics in the Form of Greatest Fixpoint\n",
      "Transfinite semantics in the form of greatest fixpoint\n",
      "Translation Correctness for First-Order Object-Oriented Pattern Matching\n",
      "Two formal semantics of a subset of the paderborn university bsplib\n",
      "Type soundness proofs with definitional interpreters\n",
      "Type-Based Amortized Resource Analysis with Integers and Arrays\n",
      "Type-based amortized resource analysis with integers and arrays*\n",
      "Typer la désérialisation sans sérialiser les types\n",
      "Typing dynamic languages–a review\n",
      "Varieties of Static Analyzers: A Comparison with ASTREE\n",
      "Verification of Programs with Pointers in SPARK\n",
      "Verified Optimizations for Functional Languages\n",
      "Well-founded Functions and Extreme Predicates in Dafny: A Tutorial\n",
      "Well-founded Functions and Extreme Predicates in Dafny: A Tutorial.\n",
      "Why Proof-Theory Matters in Specification-Based Testing\n",
      "doctorat\n",
      "emes de contr^ ole-commande critiques\n",
      "of a Tool Chain for Deductive Program Verication\n",
      "richford@ microsoft. com K. Rustan M. Leino leino@ microsoft. com\n",
      "基于 Clight 形式语义的代码功能描述提取\n",
      "無限の入出力を行う関数型プログラムの K 正規化の形式的検証\n",
      "재귀 타입과 합집합 타입을 위한 코인덕션 서브타이핑\n",
      "파이썬을 이용한 구조적 실행 의미구조 구현\n"
     ]
    }
   ],
   "source": [
    "for cit in sorted(cits.values(), key=lambda x: x.title): \n",
    "    print(cit.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309f719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
